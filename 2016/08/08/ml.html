<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Machine Learning classification | Mark Danovich</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Machine Learning classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The purpose of this article is to serve as a playground for demonstrating the use of some of the common supervised classification algorithms using the scikit-learn library, which is a widely used popular machine learning library with a very detailed documentation and plenty of examples." />
<meta property="og:description" content="The purpose of this article is to serve as a playground for demonstrating the use of some of the common supervised classification algorithms using the scikit-learn library, which is a widely used popular machine learning library with a very detailed documentation and plenty of examples." />
<meta property="og:site_name" content="Mark Danovich" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-08-08T07:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Machine Learning classification" />
<script type="application/ld+json">
{"description":"The purpose of this article is to serve as a playground for demonstrating the use of some of the common supervised classification algorithms using the scikit-learn library, which is a widely used popular machine learning library with a very detailed documentation and plenty of examples.","@type":"BlogPosting","url":"/2016/08/08/ml.html","headline":"Machine Learning classification","dateModified":"2016-08-08T07:00:00-05:00","datePublished":"2016-08-08T07:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2016/08/08/ml.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css" /><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Mark Danovich" /><script async src="https://www.googletagmanager.com/gtag/js?id=UA-10819841-6"></script>
<script>
  window['ga-disable-UA-10819841-6'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-10819841-6');
</script>
 




 
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement( document.body, {
      delimiters: [
        {left: "$$", right: "$$", display: true},
        {left: "[%", right: "%]", display: true},
        {left: "$", right: "$", display: false}
      ]}
    );
  });
</script>


<script>
    function wrap_img(fn) {
        if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
            var elements = document.querySelectorAll(".post img");
            Array.prototype.forEach.call(elements, function (el, i) {
                if (el.getAttribute("title")) {
                    const caption = document.createElement('figcaption');
                    var node = document.createTextNode(el.getAttribute("title"));
                    caption.appendChild(node);
                    const wrapper = document.createElement('figure');
                    wrapper.className = 'image';
                    el.parentNode.insertBefore(wrapper, el);
                    el.parentNode.removeChild(el);
                    wrapper.appendChild(el);
                    wrapper.appendChild(caption);
                }
            });
        } else { document.addEventListener('DOMContentLoaded', fn); }
    }
    window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        // add link icon to anchor tags
        var elem = document.querySelectorAll(".anchor-link")
        elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script></head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Mark Danovich</a><nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path
                            d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z" />
                    </svg>
                </span>
            </label>

            <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/snippets.html">Snippets</a></div>
        </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title p-name" itemprop="name headline">Machine Learning classification</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2016-08-08T07:00:00-05:00" itemprop="datePublished">
                Aug 8, 2016
            </time>
        </p><p>
            
            
            <a class="category-tags-link" href="/tag/machinelearning">
                machinelearning
            </a>
            
            
        </p>

        

        </header>

    <div class="post-content e-content" itemprop="articleBody">
        <p>The purpose of this article is to serve as a playground for demonstrating the use of some of the common supervised classification algorithms using the <a href="http://scikit-learn.org/" target="_blank">scikit-learn</a> library, which is a widely used popular machine learning library with a very detailed documentation
and plenty of examples.</p>

<p>We start by generating data with labels using the built-in function in scikit-learn called make_moons to create a moon shaped data set with two features (x and y coordinates) separated in to two classes with some added noise.
as shown below.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%matplotlib inline
import sklearn.datasets
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

matplotlib.style.use('ggplot') #makes plots look pretty

# Generate a dataset and plot it
np.random.seed(0)
X, y = sklearn.datasets.make_moons(200, noise=0.2)
plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)
</code></pre></div></div>

<p><img src="/assets/ml1.png" alt="png" /></p>

<p>The task of a classifier trained on this data set is to provide a decision boundary which will separate the two
classes in a generalizable way, such that future test points, unseen by the classifier and which are taken from the same distribution, will be correctly classified.</p>

<p>For visualization purposes we have the following function which plots the decision boundary,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def plot_decision_boundary(model,X,y):
    padding=0.15
    res=0.01
    
    #max and min values of x and y of the dataset
    x_min,x_max=X[:,0].min(), X[:,0].max()
    y_min,y_max=X[:,1].min(), X[:,1].max()
    
    #range of x's and y's
    x_range=x_max-x_min
    y_range=y_max-y_min
    
    #add padding to the ranges
    x_min -= x_range * padding
    y_min -= y_range * padding
    x_max += x_range * padding
    y_max += y_range * padding

    #create a meshgrid of points with the above ranges
    xx,yy=np.meshgrid(np.arange(x_min,x_max,res),np.arange(y_min,y_max,res))
    
    #use model to predict class at each point on the grid
    #ravel turns the 2d arrays into vectors
    #c_ concatenates the vectors to create one long vector on which to perform prediction
    #finally the vector of prediction is reshaped to the original data shape.
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])   
    Z = Z.reshape(xx.shape)
    
    #plot the contours on the grid
    plt.figure(figsize=(8,6))
    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    
    #plot the original data and labels
    plt.scatter(X[:,0], X[:,1], s=35, c=y, cmap=plt.cm.Spectral)
</code></pre></div></div>

<h3 id="logistic-regression">Logistic Regression</h3>
<p>The first algorithm is logistic regression, which despite the name is a classification algorithm and not a regression algorithm.
Logistic regression is a linear classifier which uses the sigmoid function with trained weights and bias to assign a probability for a given data point.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>W</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x; W,b)=\frac{1}{1+e^{-(W x+b)}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.10877em;vertical-align:-0.78733em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.2960000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span><span class="mord mathdefault mtight">x</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">b</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.78733em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

<center><img src="/assets/sigmoid.png" width="350px" /></center>

<p>Using scikit-learn we train a logistic regression classifier and plot its decision boundary</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">linear_model</span> <span class="n">import</span> <span class="n">LogisticRegression</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="k">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<p>the fitting model parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
</code></pre></div></div>

<p>Some hyper parameters of interest: <b>penalty=’l2’</b> sets the cost function type to be the square of the error as opposed to ‘l1’ which is the absolute
value. The parameter <b>C=1.0</b> controls (inversely) the regularization which is responsible for preventing over-fitting of the trained weights. Small values correspond to stronger regularization.
<br /></p>
<center><img src="/assets/logistic_class.png" /></center>
<p><br />
We see that the linear classifier found the best linear separation it could find, which however is not good since the particular dataset is not linearly separable.</p>

<h3 id="svm---support-vector-machine">SVM - Support vector machine</h3>
<p>The next algorithm is the support vector machine (svm) algorithm. SVM is also a linear classifier which tries to find the boundary
by focusing on the two closest points from different classes (support vectors) and finds the line that is equidistant to these two points
as the separation boundary. SVMs however have the kernel option which allows to represent the data in higher dimensions. This allows to separate data
that are not separable linearly in the original space, but are linearly separable in a higher dimensional space.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">svm</span> <span class="n">import</span> <span class="n">SVC</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="p">=</span><span class="s1">'linear'</span><span class="p">)</span>
<span class="n">print</span> <span class="k">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>The model training parameters:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre></div></div>

<p>Below we show the resulting boundary for a ‘linear’ and ‘rbf’ kernels.</p>

<center>
<img src="/assets/svm1.png" width="380px" />
<img src="/assets/svm2.png" width="380px" />
</center>

<p>The ‘rbf’ kernel which essentially represents the data in an infinite dimensional space, allows to separate the data well.</p>

<h3 id="nearest-neighbors">Nearest neighbors</h3>
<p>The next algorithm is the nearest neighbors algorithm, considered the most basic machine learning algorithm due to the simplicity of the idea, which is that 
the class of a given test point is determined by a majority vote of its <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> nearest neighboring points.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">neighbors</span> <span class="n">import</span> <span class="n">KNeighborsClassifier</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">=</span><span class="m">5</span><span class="p">)</span>
<span class="k">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>The model training parameters:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=10, p=2,
           weights='uniform')
</code></pre></div></div>

<p>We show the results of using different values for the number of neighbors, n_neighbors=1,10,100, demonstrating over-fitting or high variance
for a small number of neighbors, meaning that the model captures too much of the noise in the data and thus will note generalize well to unseen data points, and under-fitting or a high bias for a large number of neighbors.
Finally, a value between these two extremes, such as 10, manages to separate the data relatively well.</p>

<center>
<img src="/assets/nn1.png" width="305px" />
<img src="/assets/nn2.png" width="305px" />
<img src="/assets/nn3.png" width="305px" />
</center>

<h3 id="decision-tree">Decision tree</h3>
<p>The next algorithm is the decision tree, which uses intersecting vertical lines to create the decision boundary, allowing to obtain non-linear
decision boundaries.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">neighbors</span> <span class="n">import</span> <span class="n">KNeighborsClassifier</span>
<span class="k">from</span> <span class="n">sklearn</span> <span class="n">import</span> <span class="n">tree</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="p">=</span><span class="m">3</span><span class="p">)</span>
<span class="k">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=3,
            min_samples_split=3, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter='best')
</code></pre></div></div>

<p>Here the parameter <b>min_samples_leaf</b> controls the minimum number of samples required to be in a leaf, which helps to prevent over-fitting.</p>

<center>
<img src="/assets/tree.png" />
</center>

    </div><a class="u-url" href="/2016/08/08/ml.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My personal website and blog. Life, Science, Programming and Data Science.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/markd87" target="_blank" title="markd87"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/mmasaniad" target="_blank" title="mmasaniad"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
