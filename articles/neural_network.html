<!DOCTYPE html>
<html>
<head>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
   MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" integrity="sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==" crossorigin="anonymous">

      <link rel="stylesheet" href="/text_style.css" />


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10819841-6', 'auto');
  ga('send', 'pageview');

</script>
    <style>

      p{
        text-align: justify;
      }

      .navbar{
        border-bottom: 1px solid #d3d3d3;
      }
      .navbar{
        margin-bottom:0px;
        padding:5px;
      }

      .navbar ul{
        float:right;
        margin-top: 10px;
      }

      .navbar-brand{
        font-size:2em;
        color:#555 !important;
        line-height:37px;
      }

      .active{
        border:1px solid grey;
        background-color: none;
      }

      #banner{
        background-image: url(img/backs.jpg);
        background-position: 75% 68%;
        background-size: cover;
        background-repeat: no-repeat;
        width:100%;
        color:#eee;
        border-bottom: 1px solid #d3d3d3;
        margin-top:0px;
      }

      .bannerText{
        font-size:1.5em;
        width:60%;
        font-weight: bold;
      }

      .mail{
        margin-top:-2px;
      }
      .ic img{
        width:18px;
        margin-top:-2px;
      }

      .linkedin img{
          width:25px;
          margin-top:-2px;
      }

      #blender_img{
        text-align: center;
      }

      .footerp{
        color:#222;
        margin-bottom:5px;
        margin-top:10px; 
        text-align: center;
      }

      #footer{
        padding-top:40px;
        background-color: #f8f8f8;
        width:100%;
        text-align: center;
        border-bottom: 1px solid #d3d3d3;
      }

      .contact li{
        padding-right:10px;
      }


      .page{
        padding-top:70px;
      }

      #projects{
        margin-bottom:30px;
      }

      .break{
        clear:both;
        width:100%;
        border-bottom: 1px solid #d3d3d3;
      }


    </style>

</head>

  <body data-target=".navbar" data-offset="50">

    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container" id="navigation">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <img src="https://i1.rgstatic.net/ii/profile.image/AS%3A287718316756997%401445608800899_l/Mark_Danovich.png" style="
    height: 70px;
    border-radius: 35px;
    float: left;
    margin-right: 20px;
          ">
<!--<a class="navbar-brand" href="">MDD</a>-->
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav right">
            <li><a href="../index.html">home</a></li>
            <li ><a href="../about.html">about me</a></li>
            <li><a href="../research.html">research</a></li>
            <li><a class="active" href="../stuff.html">random</a></li>
          </ul>
        </div>
      </div>
    </nav> 

<div id='content' class='container lead' style='text-align:justify; margin-top:100px;'>
		<h1 class='special'>Artificial neural networks</h1>
	
  <p>
		The human brain is arguably the most sophisticated and complicated device ever "created". The thought of humans trying and succeeding to some extent, to understand the way the brain works is astonishing.
    The pursuit of understanding the human brain has naturally led to the formation of the field of artificial intelligence, in which people are trying to produce an intelligent entity capable of reasoning and understanding the world just as we do or even better. 
    </p>
    <p>
    Humans have created math as a way to abstract nature and make it comprehensible to their own brains. As Einstein famously pointed out, it is remarkable that mathematics, being a product of human thought, is capable of describing nature and reality so well.
    </p>
    <p>
    The human brain is composed of billions of interconnected neurons - brain cells, forming the neural network of the brain. The neurons communicate with each other though electrical pulses, governed by known Chemical and physical laws, governing the brain, and that's it, there is no extra magic, this seemingly basic framework, is capable of making us intelligent, being able to learn, think, and create. 
    In trying to create a machine that can learn in a similar way, a good starting point would be to see what the brain does and try to imitate it. A single neuron cell has a body which govern the activity of the cell, Dendrites which receive signals from other neurons, and Axons which transmit signals to other neurons through junctions called synapses.
    <p>

    <span class='subtitle'>Artificial neuron</span>
    <br>
    A simple abstraction of the above, would be a computational unit which receives some input from multiple sources, sums the inputs with some specific weights for each input, and perhaps an overall bias as well. This summation then goes through some activation function which determines the output of the neuron based on its input, which serves as the input for other neurons.
    <br>
    <div class='figure'>
      <img src='images/simple.png' width='40%'>
    </div>
    <br>
    We can imagine multiple layers of such artificial neurons interconnected, with the activation of one layer of neurons influencing the activation of the next, forming an artificial neural network.
    Remarkably, using a non-linear activation function makes such a deep (more than two layers) neural network a "universal function approximater", i.e. it can approximate any function from the inputs to the output by appropriately tuning it's weights.
    </p>
    <p>
    <span class='subtitle'>Learning</span>
    <br>
    The question is then, how does a neural network learn? At this point this question becomes a problem in optimization. A network's output for a given input is uniquely determined by its set of weights and biases, forming a multidimensional space with the different parameters as its axis. Given the desired output of the network, we have some loss or cost function which determines how far is the network from outputting the desired value. Therefore the task of learning is reduced to the task of minimizing this loss function through modifying the weights and biases connecting the artificial neurons and determining their outputs.
</p>
<p>
    A method to do just that is called gradient descent, which as the name implies, one finds the gradient of the loss function with respect to the different parameters, which gives the direction in which the loss function increases, therefore one needs to descend in the opposite direction dictated by the gradient, until it reaches close to the minimum of the loss function, at which point the gradient becomes close to zero and learning stops. One might then wonder what guarantees finding the minimum and not ending up in local minimum giving bad parameters. Remarkably, the high dimensionality of most learning tasks comes to the rescue. whereas in two or three dimensions, local minimas are quite common, once the number of dimensions increases rapidly, it becomes more unlikely to come across such local minimum, because that would require simultaneously all the partial derivatives of the loss function with respect to all parameters to be zero, therefore it is more likely that there will always be at least one direction in which one can go so as to keep minimizing the loss function until one gets a satisfactory training accuracy. Note that in principle, given infinite time the network could potentially learn the training set perfectly, however this results in over-fitting, which means that the network will not generalize well to previously unseen data.
    In principle one can compute the gradients numerically, however this is computationally highly inefficient. Luckily there is an analytical method to find the gradients called backpropagation. But first I demonstrate how to obtain the output from a neural network by forward propagation, which will help in deriving the backpropagation method for learning using gradient descent.
    </p>
    <p>
    <span class='subtitle'>Forward propagation</span>
    <br>
    Forward propagation means propagating the inputs through the network until we reach the output. 

    <div class='figure'>
      <img src='images/nn.png' width='40%'/>
    </div>

    (a function I wrote for generating images of neural network graphs with given number of layers and nodes can be found <a href='https://github.com/markd87/markd87.github.io/blob/master/articles/nn_graph.ipynb' target="_blank">here</a>).
    <br>
    <br>
    To demonstrate the feed-forward network, we consider a network with one hidden layer (the smallest possible deep neural network). We also use vectorized form for the equations in order to simplify notation and allow for efficient implementation.<br>
    Denoting the input as a matrix of size $n_{input}\times n_{data}$ where we have $n_{data}$ points in a given batch (mini-batch) where each data point is a vector of $n_{input}$ elements. The limit of a single data point is called stochastic gradient descent which means we update the parameters for each data point in the training set which can be noisy but fast, whereas the limit of a very big batch of data means that we have a better approximations to the true gradient of the loss function but requires more operations. (a stack exchange <a href='https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent' target="_blank">answer</a> on pros and cons of the two limits.)<br>

    Next we have a hidden layer with $n_{hidden}$ nodes. The hidden layer is connected to the inputs with a weight matrix $W_{hidden}$ of size $n_{hidden}\times n_{input}$, such that $W_{ij}$ is the weight connecting input node $i$ to hidden note $j$. Additionally a bias vector ${\bf b}_{hidden}$ of size $n_{hidden}$.<br>
    Finally, we have an output layer with $n_{output}$ nodes, which can denote either different classes in a supervised classification problem, or real values in a regression type problem. The output layer is similarly connected to the hidden layer with a weight matrix $W_{output}$ of size $n_{output}\times n_{hidden}$, and a bias vector ${\bf b}_{output}$ for the output nodes of size.

    \[
    \begin{split}
    &z_{hidden}=W_{hidden}\cdot X_{input}+b_{hidden}, \quad (n_{hidden}\times n_{data})
    \\
    &a_{hidden}=f(z_{hidden}), 
    \\
    &z_{out} = W_{output}\cdot a_{hidden}+b_{output}, \quad (n_{output}\times n_{data})
    \\
    &y_{out} = f(z_{out}), \quad (n_{output}\times n_{data}).
    \end{split}
    \]

    The output given by $f(z_{out})$ is then a matrix with a column for each input data point. In parenthesis I state the output dimensions in each step.
    <br>
    (* note that the activation function for the output layer can be chosen differently in principle, in particular for classification problems with multiple classes, one uses the soft-max activation $\frac{e^{z_{out}}}{\sum_{c} {e^{z_c}}}$ in the output layer, where the summation is over all the classes in the classification problem).

    </p>
    <br>

    <span class='subtitle'>Backpropagation</span>
    <p>
    In order to know how a given weight influences the output, which is given by the gradient of the output with respect to a given weight, one therefore needs to go backwards from the output, and chain together the derivatives of the intermediate outputs from each layer. Also known as the chain rule. We start with a loss function $L(X,W,b)$ which is a function of the input, the weights and biases. The gradient descent minimization process updates the parameters at each learning step according to,
    \[
    \begin{split}
        &W:= W -\eta \nabla_{W}L(X,W,b)
        \\
        &b:= b - \eta \nabla_{b}L(X,W,b)
    \end{split}
    \]
    where $\eta$ is the learning rate. When doing a mini-batch gradient descent, the "real" gradient is essentially approximated by the gradient of the "mini"-loss function.
    <br>
    To obtain the gradients we imploy the chain rule,
    \[


    \]




    </p>



</div>



<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>

</body>


</html>
